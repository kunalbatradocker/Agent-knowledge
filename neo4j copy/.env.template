# === Auth ===
JWT_SECRET=change-this-to-a-random-string
JWT_EXPIRY=15m
REFRESH_TOKEN_EXPIRY=7d
ADMIN_EMAIL=admin@admin.com
ADMIN_PASSWORD=Admin@1234
# Account lockout: lock after N failed attempts for M seconds
MAX_FAILED_ATTEMPTS=5
LOCKOUT_DURATION_SECONDS=900
# Rate limiting: max login attempts per IP within window
LOGIN_RATE_LIMIT_WINDOW=900
LOGIN_RATE_LIMIT_MAX=15

# === Database Configuration (used by docker-compose) ===
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=neo4j@123
NEO4J_DATABASE=neo
REDIS_URL=redis://localhost:6379
GRAPHDB_URL=http://localhost:7200
GRAPHDB_REPOSITORY=knowledge_graph_1

# === Trino (Federated Query Engine) ===
TRINO_URL=http://localhost:8080
TRINO_USER=trino
TRINO_CATALOG_PATH=/etc/trino/catalog

# === Server ===
PORT=5002
CLIENT_PORT=3000
# Comma-separated allowed origins (leave empty to allow all in dev)
# CORS_ORIGIN=http://localhost:3000,https://yourdomain.com

# === LLM Provider: 'bedrock', 'openai', or 'ollama' ===
LLM_PROVIDER=ollama

# --- Bedrock (if LLM_PROVIDER=bedrock) ---
# AWS_REGION=us-west-2
# BEDROCK_MODEL_ID=anthropic.claude-3-sonnet-20240229-v1:0
# AWS_BEARER_TOKEN_BEDROCK=<your-bearer-token>

# --- OpenAI (if LLM_PROVIDER=openai) ---
# OPENAI_API_KEY=sk-your-key-here
# OPENAI_MODEL=gpt-4o-mini

# --- Ollama / Local LLM (default, also used as fallback) ---
LLM_FALLBACK_ENABLED=true
LOCAL_LLM_BASE_URL=http://localhost:11434/v1
LOCAL_LLM_MODEL=gemma3:4b
LOCAL_EMBEDDING_MODEL=nomic-embed-text

# === Processing ===
LLM_PARALLELISM=5
MAX_CHUNKS_PER_DOC=50
EMBEDDING_PARALLELISM=10
