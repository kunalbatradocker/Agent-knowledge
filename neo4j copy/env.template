# GraphDB Configuration (Primary semantic database)
GRAPHDB_URL=http://localhost:7200
GRAPHDB_REPOSITORY=knowledge_graph_1

# Neo4j Database Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your_password_here
NEO4J_DATABASE=neo

# Redis Configuration (for vector embeddings)
REDIS_URL=redis://localhost:6379

# Server Configuration
PORT=5002
CLIENT_PORT=3000
REACT_APP_API_PORT=5002

# LLM Configuration - Choose ONE option below

# Option 1: Local LLM (Ollama/LM Studio) - Recommended
USE_LOCAL_LLM=true
LOCAL_LLM_BASE_URL=http://localhost:11434/v1
LOCAL_LLM_MODEL=gemma3:4b
LOCAL_EMBEDDING_MODEL=nomic-embed-text

# Option 2: OpenAI API (uncomment and use this instead if not using local LLM)
# USE_LOCAL_LLM=false
# OPENAI_API_KEY=sk-your-openai-api-key-here
# OPENAI_MODEL=gpt-4o-mini
# OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Processing Configuration
LLM_PARALLELISM=5          # Number of concurrent LLM calls for concept extraction
MAX_CHUNKS_PER_DOC=50      # Maximum chunks to process per document
EMBEDDING_PARALLELISM=10   # Number of concurrent embedding generations

# Embedding Configuration
EMBEDDING_DIMENSION=768

# Chunking Configuration
CHUNK_SIZE=2000
CHUNK_OVERLAP=300

# Graph RAG Configuration
# Set to 0 or -1 to process ALL chunks (warning: slow with local LLMs)
MAX_CHUNKS_FOR_EXTRACTION=20
MAX_CONTEXT_CHUNKS=5
MAX_GRAPH_NODES=20
